{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0iw8biSQHGYU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load your DataFrame\n",
        "df = pd.read_csv('../data/modern_Data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "collapsed": true,
        "id": "s28wYIeiHGYZ",
        "outputId": "e93e1de2-cacf-4bb5-df3e-0ea30978603f"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xFfDJ_IHGYb",
        "outputId": "c7f9b136-23d1-4844-8408-17acb0b2f57f"
      },
      "outputs": [],
      "source": [
        "print(\"Checking for NaN values in the 'Message' column:\")\n",
        "nan_count = df['Message'].isnull().sum()\n",
        "print(f\"Number of NaN values in 'Message' column: {nan_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy6k80CZHGYc",
        "outputId": "cbb442e3-6d73-47c6-8b5d-80aa365ebd66"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['Message'])\n",
        "\n",
        "# Print the shape of the dataset after dropping NaN values in the \"Message\" column\n",
        "print(f\"Dataset shape after dropping NaN values in 'Message' column: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "collapsed": true,
        "id": "jPCvNFeZHGYd",
        "outputId": "610e57c4-4dfb-4676-a30b-b2de68c00a1d"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "collapsed": true,
        "id": "obXIgmfSHGYe",
        "outputId": "00702635-aa01-4867-beb5-0c0ac80fc5a0"
      },
      "outputs": [],
      "source": [
        "message_df=df['Message']\n",
        "message_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "collapsed": true,
        "id": "FgMkjpzzQE4m",
        "outputId": "2ff8d5db-6046-4b30-b5e9-0e5ebe63de26"
      },
      "outputs": [],
      "source": [
        "message_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj8GA0N7gq-k"
      },
      "source": [
        "Labeling a subset of the dataset in CoNLL format for Named Entity Recognition (NER) involves tagging each word in the text with its corresponding entity label. The goal is to annotate tokens as entities such as **Product**, **Price**, and **Location** in Amharic text. Here's how you can approach the task:\n",
        "\n",
        "### 1. **Understand the CoNLL Format**\n",
        "   - In the **CoNLL format**, each word (token) from the text is placed on a separate line followed by its **entity label**.\n",
        "   - Blank lines separate different sentences or messages.\n",
        "   - Common entity labels include:\n",
        "     - **B-Product** (Beginning of a product)\n",
        "     - **I-Product** (Inside a product)\n",
        "     - **B-LOC** (Beginning of a location)\n",
        "     - **I-LOC** (Inside a location)\n",
        "     - **B-PRICE** (Beginning of a price)\n",
        "     - **I-PRICE** (Inside a price)\n",
        "     - **O** (Outside of any entity)\n",
        "\n",
        "### 2. **Set Up the Data**\n",
        "   - You will work on the **Message** column from the dataset. This column contains text describing various products, prices, and locations.\n",
        "\n",
        "### 3. **Tokenize the Text**\n",
        "   - **Tokenization** is the process of splitting the text into individual words or tokens.\n",
        "   - You can use libraries like **spaCy**, **NLTK**, or manual splitting methods to break the Amharic text into individual words.\n",
        "\n",
        "### 4. **Annotate the Entities**\n",
        "   - Manually go through the tokens and annotate them using the following rules:\n",
        "     - The **first word** of a product name, location, or price gets a **B-** prefix (e.g., **B-Product**, **B-LOC**, **B-PRICE**).\n",
        "     - Any subsequent word within the same entity is tagged with the **I-** prefix (e.g., **I-Product**, **I-LOC**, **I-PRICE**).\n",
        "     - Words that are not part of any entity are labeled **O**.\n",
        "   \n",
        "#### Example:\n",
        "If you have the following message:  \n",
        "\"አዲሱ የህጻን ቦትል በ 200 ብር ተሽጠዋል\"\n",
        "\n",
        "1. **Tokenized text:**\n",
        "   - አዲሱ, የህጻን, ቦትል, በ, 200, ብር, ተሽጠዋል\n",
        "\n",
        "2. **Labeled in CoNLL format:**\n",
        "   ```plaintext\n",
        "   አዲሱ O\n",
        "   የህጻን B-Product\n",
        "   ቦትል I-Product\n",
        "   በ B-PRICE\n",
        "   200 I-PRICE\n",
        "   ብር I-PRICE\n",
        "   ተሽጠዋል O\n",
        "   ```\n",
        "\n",
        "### 5. **Repeat for 30-50 Messages**\n",
        "   - Label **30-50 messages** in this format, ensuring that each entity in the text is annotated according to the rules.\n",
        "\n",
        "### 6. **Save the Output**\n",
        "   - Once all the annotations are complete, save the labeled dataset in a **plain text file** (e.g., **labeled_data.txt**).\n",
        "   - Ensure the format follows the CoNLL standard: **one token per line**, with its label separated by a space, and blank lines between each message.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LusP3nbQqR38"
      },
      "source": [
        "# Preprocessing: Removing Emojies, blank lines, and unwanted texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mVNeYtRkywu",
        "outputId": "756ecf79-94e1-47b1-febe-c17f370f3e13"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "def load_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "# Define a function to remove emojis\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE\n",
        "    )\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Function to remove blank lines from the dataset\n",
        "def remove_blank_lines(df, column_name):\n",
        "    # Remove rows where the message is either NaN or only contains whitespace\n",
        "    df[column_name] = df[column_name].replace(r'^\\s*$', pd.NA, regex=True)\n",
        "    df = df.dropna(subset=[column_name])\n",
        "    return df\n",
        "\n",
        "# Function to remove unwanted texts from messages\n",
        "def remove_unwanted_texts(df, column_name, unwanted_texts):\n",
        "    # Iterate through each row and remove unwanted texts and double quotes\n",
        "    for index, row in df.iterrows():\n",
        "        message = row[column_name]\n",
        "        if isinstance(message, str):\n",
        "            # Remove unwanted texts\n",
        "            for unwanted_text in unwanted_texts:\n",
        "                if unwanted_text in message:\n",
        "                    message = message.replace(unwanted_text, \"\")\n",
        "            # Remove double quotes\n",
        "            message = message.replace('\"', \"\")\n",
        "            df.at[index, column_name] = message.strip()\n",
        "    return df\n",
        "\n",
        "# Save the cleaned dataset to a new file\n",
        "def save_cleaned_dataset(df, output_file):\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Cleaned dataset saved to {output_file}\")\n",
        "\n",
        "# Main function\n",
        "def main(input_file, output_file, column_name, unwanted_texts):\n",
        "    df = load_dataset(input_file)\n",
        "\n",
        "    # Handle Missing Values\n",
        "    df = df.dropna(subset=['Message'])\n",
        "\n",
        "    # Apply the function to the 'Message' column\n",
        "    df['Message'] = df['Message'].apply(remove_emojis)\n",
        "\n",
        "    # Remove blank lines (or rows with empty messages)\n",
        "    df = remove_blank_lines(df, column_name)\n",
        "\n",
        "    # Clean the messages by removing unwanted texts and double quotes\n",
        "    cleaned_df = remove_unwanted_texts(df, column_name, unwanted_texts)\n",
        "\n",
        "    # Save the cleaned dataset to a new CSV file\n",
        "    save_cleaned_dataset(cleaned_df, output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the input file and output file paths\n",
        "    input_file = \"../data/modern_Data.csv\"\n",
        "    output_file = \"../data/cleaned_dataset.csv\"\n",
        "\n",
        "    # Define the column containing the messages\n",
        "    column_name = \"Message\"\n",
        "\n",
        "    # List of unwanted texts to remove from the messages\n",
        "    unwanted_texts = [\n",
        "        \"ቴሌግራምt.me/modernshoppingcenter\",\n",
        "        '\"በአዲስ ነገረ ሁሌም ቀዳሚዏች ነን\"',\n",
        "        \"t.me/modernshopping1\",\n",
        "        \"t.me/modernshopping2\",\n",
        "        \"በስራችን ላይ ቅሬታ ካለዎት ብቻ በዚህ ስልክ ደዉለዉ ያሳዉቁን።\",\n",
        "        \"0956415152\",\n",
        "        \"0924743736\",\n",
        "        \"0974978584\",\n",
        "        '\"በሞደርን እቃወዏች ሂወትዎን',\n",
        "        ' ሞደርናይዝ ያድርጉ\"',\n",
        "        'የመረጡትን እቃ ለማዘዝ ከታች ባለዉ የቴሌግራም አድራሻ ይላኩልን',\n",
        "        'ተጀመረ ተጀመረ ተጀመረ',\n",
        "        'ልዩ እዉነተኛ የበዓል ቅናሽ',\n",
        "        'ከነሐሴ 29 እስከ መስከረም 7 ድረስ የሚቆይ እዉነተኛ ቅናሽ አድርገናል።',\n",
        "        'ለክፍለሀገር ደንበኞቻችን ባሉበት ሐገር በመናሐሪያ እንልካለን።',\n",
        "    ]\n",
        "\n",
        "    # Run the main function\n",
        "    main(input_file, output_file, column_name, unwanted_texts)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUGqZ8Z5qg83"
      },
      "source": [
        "# Labelling the dataset: Tokenizing and Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6kjM-x12qPYV",
        "outputId": "9e5cb624-9a1b-4fb3-ca99-1902a936b8b2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "def load_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "# Function to tokenize the Amharic text (a simple split based on space)\n",
        "def tokenize_message(message):\n",
        "    # Convert to string to avoid issues with float/NaN values\n",
        "    if isinstance(message, str):\n",
        "        return message.split()\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Manual annotation function\n",
        "def annotate_message(tokens):\n",
        "    labeled_tokens = []\n",
        "    print(\"\\nStart labeling each token:\")\n",
        "    for token in tokens:\n",
        "        print(f\"Token: {token}\")\n",
        "        label = input(\"Enter label (B-Product, I-Product, B-LOC, I-LOC, B-PRICE, I-PRICE, O): \")\n",
        "        labeled_tokens.append((token, label))\n",
        "    return labeled_tokens\n",
        "\n",
        "# Save the labeled data in CoNLL format\n",
        "def save_to_conll(labeled_data, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for message in labeled_data:\n",
        "            for token, label in message:\n",
        "                f.write(f\"{token} {label}\\n\")\n",
        "            f.write(\"\\n\")  # Blank line to separate messages\n",
        "\n",
        "# Main function to annotate dataset\n",
        "def main(file_path, output_file, start_row, num_rows):\n",
        "    df = load_dataset(file_path)\n",
        "\n",
        "    labeled_data = []\n",
        "\n",
        "    # Subset the dataframe based on the start_row and num_rows\n",
        "    df_subset = df.iloc[start_row : start_row + num_rows]\n",
        "\n",
        "    # Assuming the dataset has a 'Message' column\n",
        "    for index, row in df_subset.iterrows():\n",
        "        message = row['Message']\n",
        "\n",
        "        # Tokenize the message (handle non-string cases)\n",
        "        tokens = tokenize_message(message)\n",
        "\n",
        "        if tokens:  # Skip messages that couldn't be tokenized (empty or non-string)\n",
        "            print(f\"\\nMessage {index + 1}: {message}\")\n",
        "\n",
        "            # Annotate tokens\n",
        "            labeled_tokens = annotate_message(tokens)\n",
        "\n",
        "            # Append labeled tokens\n",
        "            labeled_data.append(labeled_tokens)\n",
        "\n",
        "    # Save the annotated data in CoNLL format\n",
        "    save_to_conll(labeled_data, output_file)\n",
        "    print(f\"\\nAnnotated data saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Provide the path to the dataset and the output file\n",
        "    dataset_path = \"../data/cleaned_dataset.csv\"\n",
        "    output_file = \"../data/labeled_data.txt\"\n",
        "\n",
        "    # Specify the starting row and number of rows to process\n",
        "    start_row = int(input(\"Enter the starting row: \"))\n",
        "    num_rows = int(input(\"Enter the number of rows to label: \"))\n",
        "\n",
        "    main(dataset_path, output_file, start_row, num_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "eda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
